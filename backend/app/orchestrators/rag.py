"""Retrieval-Augmented Generation (RAG) â€“ minimal Gemini streaming orchestrator.

ì´ ëª¨ë“ˆì€ Google Gemini ChatSession ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ
ì‘ë‹µí•˜ê¸° ìœ„í•œ ê°€ìž¥ ìž‘ì€ ë‹¨ìœ„ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë³µìž¡í•œ JSON 2-phase, í”Œë ˆì´ì–´
í†µê³„, ë ˆê±°ì‹œ OpenAI í˜¸í™˜ ë¡œì§ì„ ëª¨ë‘ ì œê±°í•˜ê³  ë‹¤ìŒ ì±…ìž„ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.

1. ChatSession ë¡œë“œ/ìƒì„±   (select_model + system prompt ê²°ì • í¬í•¨)
2. llm_session_pool.get_chat() ìœ¼ë¡œ Gemini Chat ê°ì²´ íšë“
3. chat.send_message_stream(parts) í˜¸ì¶œ í›„ (ChatSession, AsyncIterator[str]) ë°˜í™˜

ì´ë ‡ê²Œ ë‹¨ìˆœí™”í•¨ìœ¼ë¡œì¨ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ RagPipeline.run_stream() í•˜ë‚˜ë¡œ
ì§‘ì•½í•˜ê³ , ë¼ìš°í„°/ì„œë¹„ìŠ¤ ê³„ì¸µì€ í† í° ìŠ¤íŠ¸ë¦¼ì„ ì „ë‹¬/ì €ìž¥í•˜ëŠ” ì¼ì—ë§Œ ì§‘ì¤‘í• 
ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
"""
from __future__ import annotations

import asyncio
import uuid
import os
from dataclasses import dataclass
from typing import AsyncIterator, List, Optional, Tuple
from app.services.genai_client import genai, client as genai_client, types

# Ensure API key configured early

from app.models.chat_session import ChatSession
from app.models.db import get_session
from app.services import prompt as prompt_service
from app.services.llm_session_pool import get_chat
from concurrent.futures import ThreadPoolExecutor

# ì „ì—­ í•œì • ThreadPool â€“ ë¬´ì œí•œ ìŠ¤ë ˆë“œ ìŠ¤í° ë°©ì§€
_STREAM_EXECUTOR = ThreadPoolExecutor(
    max_workers=int(os.getenv("STREAM_WORKERS", "64"))
)
# ---------------------------------------------------------------------------
# Helper â€“ ëª¨ë¸ ì„ íƒ
# ---------------------------------------------------------------------------

def select_model(game: Optional[str]) -> str:  # noqa: D401
    """ê²Œìž„ ì¢…ë¥˜ì— ë”°ë¼ LLM ëª¨ë¸ëª…ì„ ì„ íƒí•œë‹¤."""

    if game and game.lower() in {"lol", "pubg"}:
        return "gemini-2.5-flash"
    return "gemini-2.5-flash-lite"


# ---------------------------------------------------------------------------
# Helper â€“ flatten contents for token calc / debug
# ---------------------------------------------------------------------------
from typing import List, Dict, Any

def _flatten_contents(contents: List[Dict[str, Any]]) -> str:  # noqa: D401
    """parts ì•ˆì˜ plain text ë¥¼ ëª¨ë‘ ì´ì–´ë¶™ì—¬ ë””ë²„ê·¸/í† í°ê³„ì‚°ìš© ë¬¸ìžì—´ ë°˜í™˜"""
    buf: List[str] = []
    for c in contents:
        for p in c.get("parts", []):
            if isinstance(p, str):
                buf.append(p)
            elif isinstance(p, dict) and "text" in p:
                buf.append(str(p["text"]))
    return "\n".join(buf)

# ---------------------------------------------------------------------------
# Main Pipeline â€“ minimal streaming
# ---------------------------------------------------------------------------

@dataclass
class RagPipeline:  # noqa: D101 â€“ simple wrapper
    """Gemini ê¸°ë°˜ RAG íŒŒì´í”„ë¼ì¸ (ìŠ¤íŠ¸ë¦¬ë° ì „ìš©)."""

    async def run_stream(
        self,
        *,
        question: str,
        user_id: str,
        plan_tier: str = "free",  # plan_tier ëŠ” quota ì²´í¬ìš© â€“ í˜„ìž¬ ë¡œì§ì—ì„  ë¯¸ì‚¬ìš©
        game: str | None = None,
        prompt_type: str | None = None,
        images: List[types.Part] | None = None,
        session_id: str | None = None,
    ) -> Tuple[ChatSession, AsyncIterator[str]]:
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìœ„í•œ ë‹¨ì¼ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸.

        Parameters
        ----------
        question : str
            ì‚¬ìš©ìžì˜ í˜„ìž¬ ì§ˆë¬¸ ë¬¸ìžì—´.
        user_id : str
            Google sub (unique user id).
        plan_tier : str, optional
            ìš”ê¸ˆì œ ì •ë³´(ì‚¬ìš©ëŸ‰ ì²´í¬ ìš©ë„) â€“ í˜„ìž¬ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ.
        game : str | None, optional
            ê²Œìž„ íƒ€ìž…(lol | pubg ë“±). ëª¨ë¸ ì„ íƒì— í™œìš©.
        prompt_type : str | None, optional
            ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¢…ë¥˜(generic | lol | pubg â€¦).
        images : list[types.Part] | None, optional
            ì´ë¯¸ì§€ Part ë¦¬ìŠ¤íŠ¸ (multimodal ìž…ë ¥). None ì´ë©´ í…ìŠ¤íŠ¸ ì „ìš©.
        session_id : str | None, optional
            ê¸°ì¡´ ì„¸ì…˜ ID. ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±.

        Returns
        -------
        tuple(ChatSession, AsyncIterator[str])
            DB ì„¸ì…˜ í–‰ê³¼ Gemini token ìŠ¤íŠ¸ë¦¼.
        """

        # -------------------------------------------------------------------
        # 1) ChatSession ë¡œë“œ/ìƒì„±
        # -------------------------------------------------------------------
        async with get_session() as db:
            sess: ChatSession | None = None
            if session_id:
                sess = await db.get(ChatSession, session_id)
            if not sess:
                # ëª¨ë¸ & ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê²°ì •
                model = select_model(game)
                try:
                    p_enum = (
                        prompt_service.PromptType(prompt_type)  # type: ignore[arg-type]
                        if prompt_type
                        else prompt_service.PromptType.generic
                    )
                except ValueError:
                    p_enum = prompt_service.PromptType.generic

                sys_prompt = prompt_service.get_system_prompt(p_enum)

                # Gemini SDK v1 â€“ client ê¸°ë°˜ìœ¼ë¡œ ë³€ê²½í–ˆìœ¼ë¯€ë¡œ ë¯¸ë¦¬ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“¤ í•„ìš” ì—†ìŒ.
                # ðŸ“Œ context cachingì€ ë‹¹ìž¥ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                cache_id: str | None = None

                sess = ChatSession(
                    id=str(uuid.uuid4()),
                    user_google_sub=user_id,
                    model=model,
                    system_prompt=sys_prompt,
                    context_cache_id=cache_id,
                )
                db.add(sess)
                await db.commit()

        # -------------------------------------------------------------------
        # 2) ì´ì „ ëŒ€í™” history ë¡œë“œ (generate_content ìš©)
        # -------------------------------------------------------------------
        from app.services.llm_session_pool import get_history_contents, _DEFAULT_TOOLS

        history_contents = await get_history_contents(sess)

        # -------------------------------------------------------------------
        # 3) ì‚¬ìš©ìž ìž…ë ¥ íŒŒíŠ¸ êµ¬ì„± (multimodal)
        # -------------------------------------------------------------------
        user_parts: list = list(images) if images else []
        user_parts.append(question)
        history_contents.append({"role": "user", "parts": user_parts})

        # -------------------------------------------------------------------
        # 4) Gemini generate_content(ìŠ¤íŠ¸ë¦¬ë°) í˜¸ì¶œ
        # -------------------------------------------------------------------
        # Google GenAI SDK v1 ì‚¬ìš© â€“ Client ê¸°ë°˜ í˜¸ì¶œë¡œ ë³€ê²½
        client = genai_client
        gen_conf = {
            "candidate_count": 1,
            "max_output_tokens": 5000,
        }

        # --- Blocking -> Async ë³€í™˜ -----------------------------------------
        import asyncio
        import structlog
        logger = structlog.get_logger(__name__)

        loop = asyncio.get_running_loop()
        queue: asyncio.Queue[str | None] = asyncio.Queue()

        logger.info("gemini_request_init", history_len=len(history_contents), prompt_preview=_flatten_contents(history_contents)[:200])

        def _worker() -> None:
            try:
                for chunk in client.models.generate_content(
                    model=sess.model,
                    contents=history_contents,
                    stream=True,
                    tools=_DEFAULT_TOOLS,
                    generation_config=types.GenerationConfig(**gen_conf),
                ):
                    text = getattr(chunk, "text", None)
                    if text:
                        logger.debug("gemini_chunk", text=text)
                        loop.call_soon_threadsafe(queue.put_nowait, text)
            except Exception as e:
                logger.error("gemini_stream_error", err=str(e), exc_info=True)
            finally:
                loop.call_soon_threadsafe(queue.put_nowait, None)

        # ì „ì—­ executor ìž¬í™œìš© (llm ëª¨ë“ˆê³¼ ë™ì¼ ë¡œì§)
        _STREAM_EXECUTOR.submit(_worker)

        async def _iter() -> AsyncIterator[str]:  # noqa: D401
            while True:
                item = await queue.get()
                if item is None:
                    break
                logger.debug("send_token_to_client", token=item)
                yield item

        return sess, _iter()