"""Retrieval-Augmented Generation (RAG) â€“ minimal Gemini streaming orchestrator.

ì´ ëª¨ë“ˆì€ Google Gemini ChatSession ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ
ì‘ë‹µí•˜ê¸° ìœ„í•œ ê°€ìž¥ ìž‘ì€ ë‹¨ìœ„ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë³µìž¡í•œ JSON 2-phase, í”Œë ˆì´ì–´
í†µê³„, ë ˆê±°ì‹œ OpenAI í˜¸í™˜ ë¡œì§ì„ ëª¨ë‘ ì œê±°í•˜ê³  ë‹¤ìŒ ì±…ìž„ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.

1. ChatSession ë¡œë“œ/ìƒì„±   (select_model + system prompt ê²°ì • í¬í•¨)
2. llm_session_pool.get_chat() ìœ¼ë¡œ Gemini Chat ê°ì²´ íšë“
3. chat.send_message_stream(parts) í˜¸ì¶œ í›„ (ChatSession, AsyncIterator[str]) ë°˜í™˜

ì´ë ‡ê²Œ ë‹¨ìˆœí™”í•¨ìœ¼ë¡œì¨ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ RagPipeline.run_stream() í•˜ë‚˜ë¡œ
ì§‘ì•½í•˜ê³ , ë¼ìš°í„°/ì„œë¹„ìŠ¤ ê³„ì¸µì€ í† í° ìŠ¤íŠ¸ë¦¼ì„ ì „ë‹¬/ì €ìž¥í•˜ëŠ” ì¼ì—ë§Œ ì§‘ì¤‘í• 
ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
"""
from __future__ import annotations

import asyncio
import uuid
import os
from dataclasses import dataclass
from typing import AsyncIterator, List, Optional, Tuple
from google import genai  # type: ignore
from google.genai import types  # type: ignore


# Ensure API key configured early

from app.models.chat_session import ChatSession
from app.models.db import get_session
from app.services import prompt as prompt_service
from concurrent.futures import ThreadPoolExecutor
import logging

logger = logging.getLogger(__name__)

# ì „ì—­ í•œì • ThreadPool â€“ ë¬´ì œí•œ ìŠ¤ë ˆë“œ ìŠ¤í° ë°©ì§€
_STREAM_EXECUTOR = ThreadPoolExecutor(
    max_workers=int(os.getenv("STREAM_WORKERS", "64"))
)
# ---------------------------------------------------------------------------
# Helper â€“ ëª¨ë¸ ì„ íƒ
# ---------------------------------------------------------------------------

def select_model(game: Optional[str]) -> str:  # noqa: D401
    """ê²Œìž„ ì¢…ë¥˜ì— ë”°ë¼ LLM ëª¨ë¸ëª…ì„ ì„ íƒí•œë‹¤."""

    if game and game.lower() in {"lol", "pubg"}:
        return "gemini-2.5-flash"
    return "gemini-2.5-flash-lite"


# ---------------------------------------------------------------------------
# Main Pipeline â€“ minimal streaming
# ---------------------------------------------------------------------------

@dataclass
class RagPipeline:  # noqa: D101 â€“ simple wrapper
    """Gemini ê¸°ë°˜ RAG íŒŒì´í”„ë¼ì¸ (ìŠ¤íŠ¸ë¦¬ë° ì „ìš©)."""

    async def run_stream(
        self,
        *,
        question: str,
        user_id: str,
        game: str | None = None,
        prompt_type: str | None = None,
        images: List[types.Part] | None = None,
        session_id: str | None = None,
    ) -> Tuple[ChatSession, AsyncIterator[str]]:
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìœ„í•œ ë‹¨ì¼ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸.

        Parameters
        ----------
        question : str
            ì‚¬ìš©ìžì˜ í˜„ìž¬ ì§ˆë¬¸ ë¬¸ìžì—´.
        user_id : str
            Google sub (unique user id).
        plan_tier : str, optional
            ìš”ê¸ˆì œ ì •ë³´(ì‚¬ìš©ëŸ‰ ì²´í¬ ìš©ë„) â€“ í˜„ìž¬ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ.
        game : str | None, optional
            ê²Œìž„ íƒ€ìž…(lol | pubg ë“±). ëª¨ë¸ ì„ íƒì— í™œìš©.
        prompt_type : str | None, optional
            ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¢…ë¥˜(generic | lol | pubg â€¦).
        images : list[types.Part] | None, optional
            ì´ë¯¸ì§€ Part ë¦¬ìŠ¤íŠ¸ (multimodal ìž…ë ¥). None ì´ë©´ í…ìŠ¤íŠ¸ ì „ìš©.
        session_id : str | None, optional
            ê¸°ì¡´ ì„¸ì…˜ ID. ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±.

        Returns
        -------
        tuple(ChatSession, AsyncIterator[str])
            DB ì„¸ì…˜ í–‰ê³¼ Gemini token ìŠ¤íŠ¸ë¦¼.
        """

        # -------------------------------------------------------------------
        # 1) ChatSession ë¡œë“œ/ìƒì„±
        # -------------------------------------------------------------------
        async with get_session() as db:
            sess: ChatSession | None = None
            if session_id:
                sess = await db.get(ChatSession, session_id)
            if not sess:
                # ëª¨ë¸ & ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê²°ì •
                model = select_model(game)
                try:
                    p_enum = (
                        prompt_service.PromptType(prompt_type)  # type: ignore[arg-type]
                        if prompt_type
                        else prompt_service.PromptType.generic
                    )
                except ValueError:
                    p_enum = prompt_service.PromptType.generic

                sys_prompt = prompt_service.get_system_prompt(p_enum)

                # Gemini SDK v1 â€“ client ê¸°ë°˜ìœ¼ë¡œ ë³€ê²½í–ˆìœ¼ë¯€ë¡œ ë¯¸ë¦¬ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“¤ í•„ìš” ì—†ìŒ.
                # ðŸ“Œ context cachingì€ ë‹¹ìž¥ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                cache_id: str | None = None

                sess = ChatSession(
                    id=str(uuid.uuid4()),
                    user_google_sub=user_id,
                    model=model,
                    system_prompt=sys_prompt,
                    context_cache_id=cache_id,
                )
                db.add(sess)
                await db.commit()

        # -------------------------------------------------------------------
        # 2) ì´ì „ ëŒ€í™” history ë¡œë“œ (generate_content ìš©)
        # -------------------------------------------------------------------
        from app.services.llm_session_pool import get_history_contents

        history_contents = await get_history_contents(sess)

        # -------------------------------------------------------------------
        # 3) ì‚¬ìš©ìž ìž…ë ¥ íŒŒíŠ¸ êµ¬ì„± (multimodal)
        # -------------------------------------------------------------------
        user_parts: list[types.Part] = list(images) if images else []
        user_parts.append(types.Part(text=question))
        history_contents.append(types.Content(role="user", parts=user_parts))

        # -------------------------------------------------------------------
        # 4) Gemini generate_content(ìŠ¤íŠ¸ë¦¬ë°) í˜¸ì¶œ
        # -------------------------------------------------------------------
        # Google GenAI SDK v1 ì‚¬ìš© â€“ Client ê¸°ë°˜ í˜¸ì¶œë¡œ ë³€ê²½
        client = genai.Client()
        grounding_tool = types.Tool(
            google_search=types.GoogleSearch()
        )

        # --- Blocking -> Async ë³€í™˜ -----------------------------------------
        loop = asyncio.get_running_loop()
        queue: asyncio.Queue[str | None] = asyncio.Queue()

        def _worker() -> None:
            try:
                response = client.models.generate_content_stream(
                    model=sess.model,
                    contents=history_contents,
                    config=types.GenerateContentConfig(
                        max_output_tokens = 5000,
                        tools=[grounding_tool],
                        system_instruction=sess.system_prompt,
                    ),
                )

                for chunk in response:
                    text = getattr(chunk, "text", None)
                    if text:
                        loop.call_soon_threadsafe(queue.put_nowait, text)
                        
            except Exception as e:
                logger.error("gemini_stream_error", err=str(e), exc_info=True)
            finally:
                loop.call_soon_threadsafe(queue.put_nowait, None)

        # ì „ì—­ executor ìž¬í™œìš© (llm ëª¨ë“ˆê³¼ ë™ì¼ ë¡œì§)
        _STREAM_EXECUTOR.submit(_worker)

        async def _iter() -> AsyncIterator[str]:  # noqa: D401
            while True:
                item = await queue.get()
                if item is None:
                    break
                yield item

        return sess, _iter()